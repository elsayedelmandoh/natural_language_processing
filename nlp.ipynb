{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus**\n",
    "+ Large collection of words or phases such as dictionary\n",
    "+ Example: Documents, web sources, database"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**token**\n",
    "+ Tokenization: process of converting TEXT to TOKENS.\n",
    "+ Tokens: Words, phases extracted from documents or web sources.\n",
    "+ Text Object: sentence or phrase or word or article.\n",
    "\n",
    "+ API ==> NLTK for more tasks such as preprocessing , vectorized representation of text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature vector**\n",
    "+ A numeric array that ML models use for training and classification/regression\n",
    "tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1- pre-processing**\n",
    "    - Raw Text (input)\n",
    "    - Noise Removal: \n",
    "    - Lexicon Normalization\n",
    "    - Object Standardization\n",
    "    - Clean Text (output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**+ Pipline:**\n",
    "- **Raw Text**\n",
    "- **Noise Removal**: Remove words elly mlha4 lazma\n",
    "    * Stopwords (i,am,the,a,he,)\n",
    "    * URLs: https://www.google.com/\n",
    "    * Punctuations: ? , ! ; . \n",
    "    * Mentions: @\n",
    "    * HTML and XML markup: <p> This is a dataset .. </ p>\n",
    "    * Twitter mark-up (names, hash tags): #aws #amazon #tesla\n",
    "    * Capitalization: Acronyms: NASA, ETA etc.\n",
    "    * Phone numbers, dates: +14256565, 12/11/2013\n",
    "    * Emojis\n",
    "    * Emoticons\n",
    "- **Word Normalization** or **Lexicon Normalization**: player,plays,played,playing = play\n",
    "    * Tokenization \n",
    "    * Lemmatization \n",
    "    * Stemming\n",
    "- **Word Standardizaion** or **Object Standardization**: ISA = Inshallah\n",
    "    * Regular\n",
    "    * Expression\n",
    "    * Lookup Tables\n",
    "- **Clean Text**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1- Noise Removal**\n",
    "+ Remove words elly mlha4 lazma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementaion Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(raw_text):\n",
    "    '''\n",
    "    create list from stopwords\n",
    "    convert text to lowercase and split it as list\n",
    "    loop on each word and remove noises\n",
    "    convert list to text\n",
    "    '''\n",
    "    stopwords = ['is','the','a','...','this','there','are','i']\n",
    "    words = raw_text.lower().split()\n",
    "    # filter stopwords from words\n",
    "    filter = [word for word in words if word not in stopwords]\n",
    "    text = \" \".join(filter)\n",
    "    return text\n",
    "\n",
    "print(remove_noise(\"hey This this hello The hal there\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Removal used built-in function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "hey hello hal\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords       # include noise words\n",
    "from nltk.tokenize import word_tokenize # split text\n",
    "\n",
    "def remove_noise(raw_data):\n",
    "    '''\n",
    "    create set from stopwords function\n",
    "    convert text to lowercase and split it as list with built-in function\n",
    "    loop on each word and remove noises\n",
    "    convert list to text\n",
    "    '''\n",
    "    stop_words = stopwords.words('english')\n",
    "    word_tokens = word_tokenize(raw_data.lower())    # split text to list\n",
    "        # filter stopwords from words\n",
    "\n",
    "    filter = [word for word in word_tokens if word not in stop_words]\n",
    "    text = \" \".join(filter)\n",
    "    return text\n",
    "\n",
    "print(remove_noise(\"hey This this hello The hal there\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2- Lexicon Normalization**\n",
    "+ Lemma: play,player,plays,played,playing = play"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.1- Tokenization**\n",
    "+ Word tokenization: function is used to split the input text into individual words and return list of strings\n",
    "\n",
    "+ sentence tokenization: function is used to split the input text into individual sentences and return list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Today, the result is The spirit desires. But the flesh is weak. Which isn't perfect. But inspires much more confidence in the English-to-Russian translation.\n",
      "\n",
      "Word Tokenize:\n",
      "['today', ',', 'the', 'result', 'is', 'the', 'spirit', 'desires', '.', 'but', 'the', 'flesh', 'is', 'weak', '.', 'which', 'is', \"n't\", 'perfect', '.', 'but', 'inspires', 'much', 'more', 'confidence', 'in', 'the', 'english-to-russian', 'translation', '.']\n",
      "Sentance Tokenize:\n",
      "['Today, the result is The spirit desires.', 'But the flesh is weak.', \"Which isn't perfect.\", 'But inspires much more confidence in the English-to-Russian translation.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize , sent_tokenize\n",
    "    \n",
    "text = open(r\"word_send_tokenize.txt\",'r').read()\n",
    "print(f\"Text:\\n{text}\")\n",
    "print(f\"Word Tokenize:\\n{word_tokenize(text.lower())}\")\n",
    "print(f\"Sentance Tokenize:\\n{sent_tokenize(text)}\") # ! or ? or ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.2- Stemming**\n",
    "+ remove all word additions: ing , er , s , es ,ly , ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "help\n",
      "commun\n",
      "play plays playing play\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() # lazm create obect from class PorterStemmer\n",
    "# method stem take one word and to_lowercase is default is True\n",
    "print(stemmer.stem(\"playing\"))                   # meaning is understandable\n",
    "print(stemmer.stem(\"HeLPing\", to_lowercase=True))# meaning is understandable\n",
    "print(stemmer.stem(\"play plays playing player\")) # meaning is not understandable, should be on word\n",
    "print(stemmer.stem(\"communication\"))             # meaning is not understandable, fixed it in lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2.3- lemmatization**\n",
    "+ get root form of word \n",
    "+ get Vocabulary (dictionary importance of words) \n",
    "+ get Morphological analysis (word structure and grammer relations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "HeLPing\n",
      "communication\n",
      "play plays playing player\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# method lemmatize take on word and \"v\" for verbs\n",
    "print(lemmatizer.lemmatize(\"playing\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"HeLPing\")) # not found to_lowercase\n",
    "print(lemmatizer.lemmatize(\"play plays playing player\")) # should be on word\n",
    "print(lemmatizer.lemmatize(\"communication\", 'v'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3- Object Standardization**\n",
    "+ ISA = Inshallah\n",
    "+ LUV = Love"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retweet direct message awesome love\n"
     ]
    }
   ],
   "source": [
    "# abbreviation\n",
    "# original\n",
    "lookup_dict = {'rt':'retweet',\n",
    "               'dm':'direct message',\n",
    "               'awsm':'awesome',\n",
    "               'luv':'love'}\n",
    "def lookup_word(text):\n",
    "    '''\n",
    "    convert text to lowercase and make split it\n",
    "    create new list to put original text in it\n",
    "    for on lookup_dict to convert abbreviation to original\n",
    "        check if key or abbreviation in lookup_dict\n",
    "            take his value (original)\n",
    "        and added this value new list\n",
    "        convert list to text \n",
    "    return text\n",
    "    '''\n",
    "    tokens = text.lower().split()\n",
    "    original= []\n",
    "    for abbreviation in tokens:\n",
    "        if abbreviation in lookup_dict:\n",
    "            abbreviation= lookup_dict[abbreviation]\n",
    "        original.append(abbreviation)\n",
    "        text= \" \".join(original)\n",
    "    return text\n",
    "    \n",
    "print(lookup_word(\"rt dm awsm luv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL STEPS PRE-PROECSSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im play soccer friend play soccer realli well play yesterday play tomorrow inshallah ill win time laugh out loud\n"
     ]
    }
   ],
   "source": [
    "import re # regular expressions: to select specific pattern\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "def preprocess(text):\n",
    "    # Noise Removal\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'\\b@\\w+\\b', '', text) # Remove mentions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "        # convert text to tokens\n",
    "    tokens= word_tokenize(text.lower())\n",
    "        # remove stop words\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stopwords\n",
    "    \n",
    "    # Word Normalization\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens] # Stemming\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatization\n",
    "\n",
    "    # Word Standardization\n",
    "    lookup_table = {\n",
    "        \"isa\": \"inshallah\",\n",
    "        \"brb\": \"be right back\",\n",
    "        \"lol\": \"laugh out loud\"\n",
    "    }\n",
    "    tokens = [lookup_table.get(token, token) for token in tokens] # Replace with lookup table\n",
    "\n",
    "    # Convert tokens back to string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "input_text = open(r\"pre-processing.txt\",'r').read()\n",
    "print(preprocess(input_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', ',', 'be right back', 'result', 'laugh out loud', 'inshallah', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Today, brb result lol isa.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "lookup_table = {\n",
    "\"isa\": \"inshallah\",\n",
    "\"brb\": \"be right back\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "}\n",
    "tokens = [lookup_table.get(token, token) for token in tokens] # Replace with lookup table\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet\n",
    "\n",
    "WordNet is a lexical `database of the English` language that relates words to one another in terms of synonyms, hypernyms, hyponyms, and more. In natural language processing (NLP), WordNet is often used to find synonyms and antonyms for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ synonym : مرادف / سني نم\n",
    "+ antonym : مضادة / انت نم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('go.n.01'),\n",
       " Synset('adam.n.03'),\n",
       " Synset('crack.n.09'),\n",
       " Synset('go.n.04'),\n",
       " Synset('travel.v.01'),\n",
       " Synset('go.v.02'),\n",
       " Synset('go.v.03'),\n",
       " Synset('become.v.01'),\n",
       " Synset('go.v.05'),\n",
       " Synset('run.v.05'),\n",
       " Synset('run.v.03'),\n",
       " Synset('proceed.v.04'),\n",
       " Synset('go.v.09'),\n",
       " Synset('go.v.10'),\n",
       " Synset('sound.v.02'),\n",
       " Synset('function.v.01'),\n",
       " Synset('run_low.v.01'),\n",
       " Synset('move.v.13'),\n",
       " Synset('survive.v.01'),\n",
       " Synset('go.v.16'),\n",
       " Synset('die.v.01'),\n",
       " Synset('belong.v.03'),\n",
       " Synset('go.v.19'),\n",
       " Synset('start.v.09'),\n",
       " Synset('move.v.15'),\n",
       " Synset('go.v.22'),\n",
       " Synset('go.v.23'),\n",
       " Synset('blend.v.02'),\n",
       " Synset('go.v.25'),\n",
       " Synset('fit.v.02'),\n",
       " Synset('rifle.v.02'),\n",
       " Synset('go.v.28'),\n",
       " Synset('plump.v.04'),\n",
       " Synset('fail.v.04'),\n",
       " Synset('go.a.01')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns = wordnet.synsets(\"go\")\n",
    "syns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that found more words `synonym` for word `go` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go.n.01'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a time for working (after which you will be relieved by someone else)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it's my go\", 'a spell of work']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "تصريف الفعل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('go.n.01.go'),\n",
       " Lemma('go.n.01.spell'),\n",
       " Lemma('go.n.01.tour'),\n",
       " Lemma('go.n.01.turn')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get synonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'felicitous', 'glad', 'happy', 'well-chosen'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return set(synonyms)\n",
    "\n",
    "synonyms = get_synonyms('happy')\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use set to remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get antonyms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unhappy'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_antonyms(word):\n",
    "    antonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "    return set(antonyms)\n",
    "\n",
    "antonyms = get_antonyms('happy')\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get synonyms and antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms:\n",
      "{'adept', 'unspoilt', 'thoroughly', 'in_effect', 'safe', 'commodity', 'well', 'salutary', 'upright', 'skilful', 'sound', 'dear', 'in_force', 'proficient', 'expert', 'good', 'honest', 'near', 'dependable', 'goodness', 'ripe', 'secure', 'honorable', 'effective', 'unspoiled', 'skillful', 'beneficial', 'undecomposed', 'soundly', 'respectable', 'serious', 'just', 'estimable', 'right', 'practiced', 'trade_good', 'full'}\n",
      "antonyms:\n",
      "{'badness', 'bad', 'ill', 'evilness', 'evil'}\n"
     ]
    }
   ],
   "source": [
    "def get_synonyms_antonyms(word):\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word): # get synonyms\n",
    "        for lemma in syn.lemmas(): # get lemmas for each synonym\n",
    "            synonyms.append(lemma.name())\n",
    "            if lemma.antonyms():\n",
    "                antonyms.append(lemma.antonyms()[0].name())\n",
    "                \n",
    "    return set(synonyms), set(antonyms)\n",
    "\n",
    "synonyms, antonyms = get_synonyms_antonyms('good')\n",
    "print(\"synonyms:\")\n",
    "print(synonyms)\n",
    "print(\"antonyms:\")\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get similarity of words using `wup_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = wordnet.synset('dog.n.01')\n",
    "word2 = wordnet.synset('cat.n.01')\n",
    "word1.wup_similarity(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two object are similar to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956521739130435"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word1 = wordnet.synset('ship.n.01')\n",
    "word2 = wordnet.synset('car.n.01')\n",
    "word1.wup_similarity(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two objects are moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = wordnet.synset('ship.n.01')\n",
    "word2 = wordnet.synset('boat.n.01')\n",
    "word1.wup_similarity(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "two objects are moving and are in sea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech \"POS\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refers to `grammatical category` of a word based on its syntactic functions and `relationships with other words` in a sentence.\n",
    "\n",
    "Identifying the part of speech of each word is a crucial step in `understanding` the `structure and meaning` of a given `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The: DT\n",
      "quick: JJ\n",
      "brown: NN\n",
      "fox: NN\n",
      "jumps: VBZ\n",
      "over: IN\n",
      "the: DT\n",
      "lazy: JJ\n",
      "dog: NN\n",
      ".: .\n"
     ]
    }
   ],
   "source": [
    "def get_pos(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "pos_tags = get_pos(sentence)\n",
    "for word, pos in pos_tags:\n",
    "    print(f\"{word}: {pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"pos_1\"](imgs/pos_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Adjective` (`JJ`): A word that describes or modifies a noun. Examples: `happy, red, tall.`\n",
    "\n",
    "\n",
    "`Noun` (`NN`): A word that represents a person, place, thing, or idea. Examples: `dog, city, happiness.`\n",
    "\n",
    "`Adverb` (`RB`): A word that modifies a verb, adjective, or other adverb and often expresses manner, time, place, or degree. Examples: `quickly, very, here.`\n",
    "\n",
    "`Pronoun` (`PRP`): A word that takes the place of a noun. Examples: `he, she, it.`\n",
    "\n",
    "`Conjunction` (`CC`): A word that connects words, phrases, or clauses. Examples: `and, but, or.`\n",
    "\n",
    "`Determiner` (`DT`): A word that introduces a noun and specifies it as something particular. Examples: `the, a, this.`\n",
    "\n",
    "`Preposition` (`IN`): A word that shows the relationship between a noun (or pronoun) and other elements in the sentence. Examples: `in, on, under.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"pos_2\"](imgs/pos_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Interjection` (`UH`): A word or phrase used to express strong emotion or sudden exclamations. Examples: `wow, oh, ouch.`\n",
    "\n",
    "`Verb` (`VB`): A word that expresses an action, occurrence, or state of being. Examples: `run, eat, is.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"pos_3\"](imgs/pos_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"pos_4\"](imgs/pos_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Chunking` is built on part of speech\n",
    "\n",
    "technique that involves identifying and grouping together adjacent words in a sentence that form a meaningful unit, known as a \"chunk.\" Unlike part-of-speech tagging, which labels individual words with their grammatical categories (such as nouns, verbs, etc.), chunking involves the extraction of larger syntactic units or phrases.\n",
    "\n",
    "chunk is typically a phrase or a group of words that serve a specific grammatical role or convey a certain meaning. Common types of chunks include noun phrases (NP), verb phrases (VP), and prepositional phrases (PP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize, RegexpParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent\n",
      " The quick brown fox jumps over the lazy dog, BBC\n",
      "pos_tags\n",
      " [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), (',', ','), ('BBC', 'NNP')]\n",
      "chunked_phrases\n",
      " [[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN')], [('fox', 'NN')], [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')], [('BBC', 'NNP')]]\n"
     ]
    }
   ],
   "source": [
    "def get_chunking(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    \n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # define pattern for chunking\n",
    "    \"\"\"\n",
    "    `NP`: This is a `label` that specifies the `type of chunk`\n",
    "    \n",
    "    `{<DT>?<JJ>*<NN>}`: This is a `regular expression pattern`\n",
    "    \n",
    "    `<DT>?`: `Zero` or `one` determiner (optional)\n",
    "\n",
    "    `<JJ>*`: `Zero` or `more` adjectives (optional)\n",
    "\n",
    "    `<NN>`: `Exactly one` noun\n",
    "    \n",
    "    `{<NNP>*}`: one or more Proper noun\n",
    "    \n",
    "    `<NN><NN>`: two noun \n",
    "\n",
    "    \"An `NP chunk` consists of an `optional determiner`, \n",
    "                    followed by `zero or more adjectives`, \n",
    "                    and finally, `exactly one noun`\n",
    "    \"\"\"\n",
    "    chunk_pattern = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}\n",
    "        {<NNP>*}\n",
    "        {<NN><NN>}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a chunk parser with the defined pattern\n",
    "    \"\"\"\n",
    "    The `RegexpParser` in NLTK (Natural Language Toolkit) is a regular-expression-based `parser \n",
    "    that allows you to define chunking patterns using regular expressions`. \n",
    "    It's useful for simple syntactic `analysis and chunking tasks` \n",
    "    where you want to define explicit patterns `based on part-of-speech tags`.\n",
    "    \"\"\"\n",
    "    chunk_parser = RegexpParser(chunk_pattern)\n",
    "    \n",
    "    # Apply the chunk parser to the part-of-speech tagged words\n",
    "    chunks = chunk_parser.parse(pos_tags)\n",
    "    \n",
    "    # Extract the chunks from the parsed result\n",
    "    chunked_phrases = [subtree.leaves() for subtree in chunks.subtrees() if subtree.label() == 'NP']  # NP is chunk pattern\n",
    "\n",
    "    return sent, pos_tags, chunked_phrases\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog, BBC\"\n",
    "sent, pos_tags, chunked_phrases = get_chunking(sentence)\n",
    "print(\"Sent\\n\", sent)\n",
    "print(\"pos_tags\\n\", pos_tags)\n",
    "print(\"chunked_phrases\\n\", chunked_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Chinking` is built on part of speech\n",
    "+ Chunking = `select`, select chunks from sentence \n",
    "+ Chinking = `remove`, remove chunks from sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chinking` is the process of `excluding` certain `tokens from a chunk` that would normally be included based on a chunking rule. \n",
    "\n",
    "In other words, it's the `reverse of chunking`. While \n",
    "\n",
    "`chunking` involves `selecting specific patterns to form chunks`, \n",
    "\n",
    "`chinking` involves `specifying patterns to exclude from a chunk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent\n",
      " The quick brown fox jumps over the lazy dog, BBC\n",
      "pos_tags\n",
      " [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), (',', ','), ('BBC', 'NNP')]\n",
      "chunked_phrases\n",
      " [[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN')], [('fox', 'NN')], [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "def get_chinking(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    \n",
    "    # Perform part-of-speech tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Define a chunking pattern with chinking\n",
    "    chunk_chink_pattern = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN>}    # Chunk Noun Phrases\n",
    "        }<VB.*>{            # Chink sequences of verbs\n",
    "        }<NNP>{             # Chink sequences of noun phrases\n",
    "        }<NN> <NN>{         # Chinking pattern to exclude consecutive nouns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a chunk parser with the defined pattern\n",
    "    chunk_chink_parser = RegexpParser(chunk_chink_pattern)\n",
    "    \n",
    "    # Apply the chunk parser to the part-of-speech tagged words\n",
    "    chunks = chunk_chink_parser.parse(pos_tags)\n",
    "    \n",
    "    chunked_phrases = [subtree.leaves() for subtree in chunks.subtrees() if subtree.label() == 'NP']\n",
    "\n",
    "    return sent, pos_tags, chunked_phrases\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog, BBC\"\n",
    "sent, pos_tags, chunked_phrases = get_chinking(sentence)\n",
    "print(\"Sent\\n\", sent)\n",
    "print(\"pos_tags\\n\", pos_tags)\n",
    "print(\"chunked_phrases\\n\", chunked_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vectorization or One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word = feature = dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"vectorization\"](imgs/vectorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    "- vector increasing when unique values\n",
    "- increasing vectors will increase demiension will increase complexity\n",
    "- not relationship between values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"count_vectorizer\"](imgs/count_vectorizer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "*********\n",
      "[[0 1 0 1]\n",
      " [1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Vectorizer = CountVectorizer()\n",
    "x = [\"Hello guys\", \"I am happy\"]\n",
    "values = Vectorizer.fit_transform(x)\n",
    "print(values)\n",
    "print('*'*9)\n",
    "values = Vectorizer.fit_transform(x).toarray()\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['am', 'guys', 'happy', 'hello'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = Vectorizer.get_feature_names_out()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>am</th>\n",
       "      <th>guys</th>\n",
       "      <th>happy</th>\n",
       "      <th>hello</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   am  guys  happy  hello\n",
       "0   0     1      0      1\n",
       "1   1     0      1      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(values, columns=names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"datasets/yelp.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   business_id  10000 non-null  object\n",
      " 1   date         10000 non-null  object\n",
      " 2   review_id    10000 non-null  object\n",
      " 3   stars        10000 non-null  int64 \n",
      " 4   text         10000 non-null  object\n",
      " 5   type         10000 non-null  object\n",
      " 6   user_id      10000 non-null  object\n",
      " 7   cool         10000 non-null  int64 \n",
      " 8   useful       10000 non-null  int64 \n",
      " 9   funny        10000 non-null  int64 \n",
      "dtypes: int64(4), object(6)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "4    3526\n",
       "5    3337\n",
       "3    1461\n",
       "2     927\n",
       "1     749\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stars\n",
       "5    3337\n",
       "1     749\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stars_1_5 = df[(df['stars'] == 1) | (df['stars'] == 5)]\n",
    "df_stars_1_5['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".values to convert series or dataframe to list or numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= df_stars_1_5['text'].values\n",
    "y = df_stars_1_5['stars'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent.  The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure.  Our waitress was excellent and our food arrived quickly on the semi-busy Saturday morning.  It looked like the place fills up pretty quickly so the earlier you get here the better.\\n\\nDo yourself a favor and get their Bloody Mary.  It was phenomenal and simply the best I\\'ve ever had.  I\\'m pretty sure they only use ingredients from their garden and blend them fresh when you order it.  It was amazing.\\n\\nWhile EVERYTHING on the menu looks excellent, I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious.  It came with 2 pieces of their griddled bread with was amazing and it absolutely made the meal complete.  It was the best \"toast\" I\\'ve ever had.\\n\\nAnyway, I can\\'t wait to go back!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replace any thing not characters to space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My wife took me here on my birthday for breakfast and it was excellent   The weather was perfect which made sitting outside overlooking their grounds an absolute pleasure   Our waitress was excellent and our food arrived quickly on the semi busy Saturday morning   It looked like the place fills up pretty quickly so the earlier you get here the better   Do yourself a favor and get their Bloody Mary   It was phenomenal and simply the best I ve ever had   I m pretty sure they only use ingredients from their garden and blend them fresh when you order it   It was amazing   While EVERYTHING on the menu looks excellent  I had the white truffle scrambled eggs vegetable skillet and it was tasty and delicious   It came with   pieces of their griddled bread with was amazing and it absolutely made the meal complete   It was the best  toast  I ve ever had   Anyway  I can t wait to go back '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('[^a-zA-Z]', ' ', x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= []\n",
    "for row in range(0,len(x)):\n",
    "    reviews = re.sub('[^a-zA-Z]', ' ', x[row])\n",
    "    reviews = word_tokenize(reviews.lower())\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words('English')\n",
    "    stop_words.remove('not')\n",
    "    reviews = [stemmer.stem(word) for word in reviews if word not in stop_words]\n",
    "    reviews = ' '.join(reviews)\n",
    "    corpus.append(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aa', 'aaa', 'aaaamaz', ..., 'zwftv', 'zwiebel', 'zzed'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vectorizer = CountVectorizer()\n",
    "x = Vectorizer.fit_transform(corpus).toarray()\n",
    "names = Vectorizer.get_feature_names_out()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12998"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaamaz</th>\n",
       "      <th>aaammmazz</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbi</th>\n",
       "      <th>...</th>\n",
       "      <th>zucca</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zuccini</th>\n",
       "      <th>zuchinni</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zupa</th>\n",
       "      <th>zuzu</th>\n",
       "      <th>zwftv</th>\n",
       "      <th>zwiebel</th>\n",
       "      <th>zzed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4081</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4082</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4084</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4086 rows × 12998 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aaa  aaaamaz  aaammmazz  aaron  ab  aback  abandon  abba  abbi  ...  \\\n",
       "0      0    0        0          0      0   0      0        0     0     0  ...   \n",
       "1      0    0        0          0      0   0      0        0     0     0  ...   \n",
       "2      0    0        0          0      0   0      0        0     0     0  ...   \n",
       "3      0    0        0          0      0   0      0        0     0     0  ...   \n",
       "4      0    0        0          0      0   0      0        0     0     0  ...   \n",
       "...   ..  ...      ...        ...    ...  ..    ...      ...   ...   ...  ...   \n",
       "4081   0    0        0          0      0   0      0        0     0     0  ...   \n",
       "4082   0    0        0          0      0   0      0        0     0     0  ...   \n",
       "4083   0    0        0          0      0   0      0        0     0     0  ...   \n",
       "4084   0    0        0          0      0   0      0        0     0     0  ...   \n",
       "4085   0    0        0          0      0   0      0        0     0     0  ...   \n",
       "\n",
       "      zucca  zucchini  zuccini  zuchinni  zumba  zupa  zuzu  zwftv  zwiebel  \\\n",
       "0         0         0        0         0      0     0     0      0        0   \n",
       "1         0         0        0         0      0     0     0      0        0   \n",
       "2         0         0        0         0      0     0     0      0        0   \n",
       "3         0         0        0         0      0     0     0      0        0   \n",
       "4         0         0        0         0      0     0     0      0        0   \n",
       "...     ...       ...      ...       ...    ...   ...   ...    ...      ...   \n",
       "4081      0         0        0         0      0     0     0      0        0   \n",
       "4082      0         0        0         0      0     0     0      0        0   \n",
       "4083      0         0        0         0      0     0     0      0        0   \n",
       "4084      0         0        0         0      0     0     0      0        0   \n",
       "4085      0         0        0         0      0     0     0      0        0   \n",
       "\n",
       "      zzed  \n",
       "0        0  \n",
       "1        0  \n",
       "2        0  \n",
       "3        0  \n",
       "4        0  \n",
       "...    ...  \n",
       "4081     0  \n",
       "4082     0  \n",
       "4083     0  \n",
       "4084     0  \n",
       "4085     0  \n",
       "\n",
       "[4086 rows x 12998 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(x, columns= names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "model = classifier.fit(x_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, ..., 5, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = classifier.predict(x_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 93, 132],\n",
       "       [  6, 995]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.41      0.57       225\n",
      "           5       0.88      0.99      0.94      1001\n",
      "\n",
      "    accuracy                           0.89      1226\n",
      "   macro avg       0.91      0.70      0.75      1226\n",
      "weighted avg       0.89      0.89      0.87      1226\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TF-IDF Term Frequency - Inverse Document Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is better than `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"tfidf\"](imgs/tfidf_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"tfidf_2\"](imgs/tfidf_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 20 sentences\n",
    "- 5 words \n",
    "- word appear in `one sentence` then tf = count_word/ len_sentance = `1/5`\n",
    "- idf = log(len_sentance/ count_word) = `log(20/1)`\n",
    "- result = tf * idf = `1/5 * log(20/1)`\n",
    "\n",
    "- special case: in case `df = 0` means count_word is 0, we `add one to df`: log(len_sentance/ count_word + 1) to `avoid division by zero`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(lst, vectorizer):\n",
    "    matrix = vectorizer.fit_transform(lst).toarray()\n",
    "    names = vectorizer.get_feature_names_out()\n",
    "    return pd.DataFrame(matrix, columns=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eats</th>\n",
       "      <th>everything</th>\n",
       "      <th>football</th>\n",
       "      <th>he</th>\n",
       "      <th>likes</th>\n",
       "      <th>much</th>\n",
       "      <th>play</th>\n",
       "      <th>so</th>\n",
       "      <th>that</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eats  everything  football  he  likes  much  play  so  that  to\n",
       "0     0           0         1   1      1     1     1   1     1   1\n",
       "1     1           1         0   1      1     0     0   0     0   1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = [\"He likes to play football so much that\", \n",
    "          \"He likes to eats everything\"]\n",
    "cv = CountVectorizer()\n",
    "create_table(test_1, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eats</th>\n",
       "      <th>everything</th>\n",
       "      <th>football</th>\n",
       "      <th>he</th>\n",
       "      <th>likes</th>\n",
       "      <th>much</th>\n",
       "      <th>play</th>\n",
       "      <th>so</th>\n",
       "      <th>that</th>\n",
       "      <th>to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.391668</td>\n",
       "      <td>0.278675</td>\n",
       "      <td>0.278675</td>\n",
       "      <td>0.391668</td>\n",
       "      <td>0.391668</td>\n",
       "      <td>0.391668</td>\n",
       "      <td>0.391668</td>\n",
       "      <td>0.278675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.533098</td>\n",
       "      <td>0.533098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379303</td>\n",
       "      <td>0.379303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       eats  everything  football        he     likes      much      play  \\\n",
       "0  0.000000    0.000000  0.391668  0.278675  0.278675  0.391668  0.391668   \n",
       "1  0.533098    0.533098  0.000000  0.379303  0.379303  0.000000  0.000000   \n",
       "\n",
       "         so      that        to  \n",
       "0  0.391668  0.391668  0.278675  \n",
       "1  0.000000  0.000000  0.379303  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_1 = [\"He likes to play football so much that\", \n",
    "          \"He likes to eats everything\"]\n",
    "tfidf = TfidfVectorizer()\n",
    "create_table(test_1, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see `TfidfVectorizer` based on `frequency` which is better than `CountVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem `TfidfVectorizer` make vectorizer on sentence not `word`, and not importance `means of words` (play= 0.391668, that= 0.391668) that take same value in sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert Word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a pre-trained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the vector for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = model['Hello'] \n",
    "print(f\"Vector for 'example_word': {word_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print `300 values` for word `Hello`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word1 = model['king']\n",
    "word2 = model['queen']\n",
    "print(np.linalg.norm(word1 - word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subtract two words from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('iphone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity('iphone', 'android')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get more words similar to `woman + king - man`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.doesnt_match(\"house garage store sea\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get words `not matching` with sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULE:\n",
    "\n",
    "similarity_rule = `A * B / ||A|| * ||B||`\n",
    "\n",
    "when cos(angle) is `large` then two words are `more` similar\n",
    "\n",
    "when cos(angle) is `small` then two words are `less` similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove : unsupervised vectorizer\n",
    "\n",
    "for search ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algoritms convert text to numbers**\n",
    "+ Bag of words\n",
    "+ TF-IDF\n",
    "+ Unigram\n",
    "+ Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text analysis using nltk.text\n",
    "from nltk.text import Text\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['two', 'plus', 'two', 'equal', 'four', ',', 'my', 'name', 'is', 'mohamed', 'salah', ',', 'RT', 'this', 'is', 'a', 'retweeted', 'tweet', 'by', 'shivam', 'Bansal', ',', 'DM', 'for', 'message', ',', 'i', 'am', 'mohamed']\n"
     ]
    }
   ],
   "source": [
    "# convert Text to Tokenize\n",
    "my_string = \"two plus two equal four , my name is mohamed salah , RT this is a retweeted tweet by shivam Bansal ,  DM for message , i am mohamed \"\n",
    "tokens = word_tokenize(my_string)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: two plus two equal four , my name...>\n"
     ]
    }
   ],
   "source": [
    "# convert Tokenize to Text\n",
    "text= Text(tokens)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['plus', 'two', 'equal', 'four', ',']\n"
     ]
    }
   ],
   "source": [
    "# make slicing\n",
    "print(tokens[1:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      " is mohamed sal\n",
      " am mohamed \n"
     ]
    }
   ],
   "source": [
    "# takes a word as input,\n",
    "# displays all the matches of the word in the text\n",
    "# width: number of characters displayed per line, default is 80 char\n",
    "# lines: number of lines displayed in the output, default is 25 line\n",
    "text.concordance(\"mohamed\", width=15,lines=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nuses a statistical measure called pointwise mutual information (PMI)\\nidentify the most significant collocations in the text.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sequence of words that together more frequently \n",
    "# display top 20 most frequent collocations.\n",
    "# collocations are sequence of words\n",
    "text.collocations() \n",
    "'''\n",
    "uses a statistical measure called pointwise mutual information (PMI)\n",
    "identify the most significant collocations in the text.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times this word \"appears\" in the text.\n",
    "text.count(\"equal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index of the \"first\" occurrence of the word in the text.\n",
    "text.index(\"two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# find words that have a similar context as the given word\n",
    "text.similar(\"four\")  #bgeb el kalma el match m3aha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Samples', ylabel='Counts'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "text = Text(reuters.words())\n",
    "print(text)\n",
    " \n",
    "text.common_contexts([\"February\",\"August\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'am', 'from')\n",
      "('am', 'from', 'egypt')\n",
      "('from', 'egypt', '.')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence = \" i am from egypt .\"\n",
    "n=3\n",
    "unigrams= ngrams(sentence.split(),n)\n",
    "for grams in unigrams:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.raw('arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.raw('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.raw(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers, like many sequence-to-sequence models, need to be trained with input sequences that predict the next word in a sequence. This requires a specific way of preparing the target sequences:\n",
    "\n",
    "- Input Sequences: The input sequences remain as they are.\n",
    "\n",
    "- Response Sequences: These sequences are used to create both the decoder input and the decoder output sequences.\n",
    "\n",
    "    + Decoder Input: The target sequences are shifted right by one position. This means that the model sees the start of a sentence and predicts the next word at each step.\n",
    "\n",
    "    + Decoder Output: This is the actual target sequence which the model should predict.\n",
    "\n",
    "Here's how this looks in practice:\n",
    "\n",
    "For a given target sequence: [START, How, are, you, doing, ?]\n",
    "\n",
    "Decoder Input: [START, How, are, you, doing]\n",
    "\n",
    "Decoder Output: [How, are, you, doing, ?]\n",
    "\n",
    "- 1.4.1- `Tokenization` using Tokenizer class and fit_on_texts method\n",
    "\n",
    "    - Convert texts to tokens \n",
    "\n",
    "- 1.4.2- `Sequences` using texts_to_sequences method\n",
    "\n",
    "    - Convert tokens to Sequences, return sequences\n",
    "\n",
    "- 1.4.3- `Padding` using pad_sequences method\n",
    "\n",
    "    - Convert Sequences to pad sequences\n",
    "    - Ensuring all sequences have the same length.\n",
    "    - Return pad_sequences\n",
    "\n",
    "- 1.4.4-  `split 'pad_sequences' array` to input_pad_sequences and target_pad_sequences\n",
    "    - If we build transformer\n",
    "    - For prepare Decoder Input and Output Sequences\n",
    "\n",
    "- 1.4.5- `Split data` using train_test_split method\n",
    "\n",
    "    - Perform the train-test split on the padded sequences with "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
